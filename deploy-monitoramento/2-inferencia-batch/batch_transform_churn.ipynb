{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Batch Transform with SageMaker Studio\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "- Image: Data Science\n",
    "- Kernel: Python 3\n",
    "- Instance type: ml.t3.medium\n",
    "\n",
    "## Background\n",
    "\n",
    "Esse notebook é baseado em notebooks anteriores onde treinamentos modelos para prever quando um cliente irá abandonar um serviço de telecomunicação. Nesse notebook, vamos treinar um modelo para fazermos inferências (predições) em batches de dados (carregados `batch_data.csv`).\n",
    "\n",
    "Esse Notebook foi adaptado do [SageMaker examples](https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_applying_machine_learning/xgboost_customer_churn/xgboost_customer_churn.ipynb)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Inicialize o ambiente e as variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install sagemaker-experiments\n",
    "import sys\n",
    "!{sys.executable} -m pip install sagemaker-experiments\n",
    "# Please restart the notebook after executing this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true,
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import boto3\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "# Get the SageMaker session and the execution role from the SageMaker domain\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = '<name-of-your-bucket>' # Update with the name of a bucket that is already created in S3\n",
    "prefix = 'demo' # The name of the folder that will be created in the S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from time import strftime\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Para essa atividade, o dataset já foi escolhido e separado em `train.csv` e `validation.csv`.\n",
    "\n",
    "Vamos enviar o dataset para o bucket S3 para que o SageMaker possa utilizá-lo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('validation.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "Nessa seção, nós vamos utilizar o SageMaker Experiments. Assim que configurarmos, nós podemos iniciar o treinamento do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an experiment\n",
    "create_date = strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "experiment_name = 'batch-transform-churn-experiment'\n",
    "experiment_description = 'A demo experiment'\n",
    "\n",
    "# Use a try-block so we can re-use an existing experiment rather than creating a new one each time\n",
    "try:\n",
    "    experiment = Experiment.create(experiment_name=experiment_name.format(create_date), \n",
    "                                   description=experiment_description)\n",
    "except ClientError as e:\n",
    "    print(f'{experiment_name} already exists and will be reused.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a trial for the experiment\n",
    "trial_name = \"batch-transform-churn-trial-2\"\n",
    "\n",
    "demo_trial = Trial.create(trial_name = trial_name.format(create_date),\n",
    "                          experiment_name = experiment_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento\n",
    "\n",
    "Vamos fazer o treinamento novamente.\n",
    "\n",
    "Precisamos especificar: onde está os nossos dados de treinamento, o caminho para o container que irá executar o algoritmo e o algoritmo a ser utilizado (junto com seus hyperparâmetros)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The location of our training and validation data in S3\n",
    "s3_input_train = TrainingInput(\n",
    "    s3_data='s3://{}/{}/train'.format(bucket, prefix), content_type='csv'\n",
    ")\n",
    "s3_input_validation = TrainingInput(\n",
    "    s3_data='s3://{}/{}/validation/'.format(bucket, prefix), content_type='csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The location of the XGBoost container version 1.5-1 (an AWS-managed container)\n",
    "container = sagemaker.image_uris.retrieve('xgboost', sess.boto_region_name, '1.5-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up experiment_config, which will be passed to the Estimator; this component will be for the training part only (later on, we'll update the TrialComponentDisplayName for the batch transform job\n",
    "experiment_config={'ExperimentName': experiment_name,\n",
    "                   'TrialName': trial_name,\n",
    "                   'TrialComponentDisplayName': 'TrainingJob'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize hyperparameters\n",
    "hyperparameters = {\n",
    "                    'max_depth':'5',\n",
    "                    'eta':'0.2',\n",
    "                    'gamma':'4',\n",
    "                    'min_child_weight':'6',\n",
    "                    'subsample':'0.8',\n",
    "                    'objective':'binary:logistic',\n",
    "                    'eval_metric':'error',\n",
    "                    'num_round':'100'}\n",
    "\n",
    "# Output path where the trained model will be saved\n",
    "output_path = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "\n",
    "# Set up the Estimator, which is training job\n",
    "xgb = sagemaker.estimator.Estimator(image_uri=container, \n",
    "                                    hyperparameters=hyperparameters,\n",
    "                                    role=role,\n",
    "                                    instance_count=1, \n",
    "                                    instance_type='ml.m4.xlarge', \n",
    "                                    output_path=output_path,\n",
    "                                    sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"fit\" executes the training job\n",
    "# We're passing in experiment_config so that the training results will be tied to the experiment\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation}, experiment_config=experiment_config) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformação Batch (Batch Transform)\n",
    "\n",
    "Agora que fizemos o treinamento do modelo, vamos utilizar para fazer predições de batches de dados. Batch Transform vai provisionar a infraestrutura necessária, e irá executar a inferência.\n",
    "\n",
    "Para essa lição, nós vamos passar os dados com `batch_data.csv`. \n",
    "\n",
    "IMPORTANTE: O dataset utilizado para fazer predições em batch não pode ter a coluna de target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read data into a dataframe\n",
    "batch_data_path = 'batch_data.csv'\n",
    "df = pd.read_csv(batch_data_path, delimiter=',', index_col=None)\n",
    "\n",
    "batch_data = df.iloc[:, 1:] # delete the target column\n",
    "batch_data.to_csv('batch_data_for_transform.csv', header=False, index = False)\n",
    "\n",
    "# Upload the new CSV file (without the target column) to S3\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'batch/batch_data_for_transform.csv')).upload_file('batch_data_for_transform.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The location of the batch data used for prediction, and location for batch output\n",
    "s3_batch_input = 's3://{}/{}/batch/batch_data_for_transform.csv'.format(bucket,prefix) \n",
    "s3_batch_output = 's3://{}/{}/batch/batch-inference'.format(bucket, prefix) \n",
    "\n",
    "# Create the Batch Transform job\n",
    "transformer = xgb.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    strategy=\"MultiRecord\",\n",
    "    assemble_with=\"Line\",\n",
    "    accept=\"text/csv\",\n",
    "    output_path=s3_batch_output\n",
    ")\n",
    "\n",
    "# Update the TrialComponentDisplay name; this is for the transform part of the trial (the previous component was for training)\n",
    "experiment_config={'ExperimentName': experiment_name,\n",
    "                   'TrialName': trial_name,\n",
    "                   'TrialComponentDisplayName': 'BatchTransformJob'}\n",
    "\n",
    "transformer.transform(s3_batch_input, content_type=\"text/csv\", split_type=\"Line\", experiment_config = experiment_config)\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the batch transform output locally\n",
    "!aws s3 cp --recursive $transformer.output_path ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first ten predictions (you can also double-click the file in the folder view to see all predictions)\n",
    "!head batch_data_for_transform.csv.out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fazendo a limpeza\n",
    "\n",
    "Nessa seção, vamos fazer a limpeza na casa e deletar nossos experimentos e modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to iterate through an experiment to delete its trials, then delete the experiment itself\n",
    "def cleanup_sme_sdk(demo_experiment):\n",
    "    for trial_summary in demo_experiment.list_trials():\n",
    "        trial = Trial.load(trial_name=trial_summary.trial_name)\n",
    "        for trial_component_summary in trial.list_trial_components():\n",
    "            tc = TrialComponent.load(\n",
    "                trial_component_name=trial_component_summary.trial_component_name)\n",
    "            trial.remove_trial_component(tc)\n",
    "            try:\n",
    "                # Comment out to keep trial components\n",
    "                tc.delete()\n",
    "            except:\n",
    "                # Trial component is associated with another trial\n",
    "                continue\n",
    "            # To prevent throttling\n",
    "            time.sleep(.5)\n",
    "        trial.delete()\n",
    "        experiment_name = demo_experiment.experiment_name\n",
    "    demo_experiment.delete()\n",
    "    print(f\"\\nExperiment {experiment_name} deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Call the function above to delete an experiment and its trials\n",
    "# Fill in your experiment name (not the display name)\n",
    "experiment_to_cleanup = Experiment.load(experiment_name='batch-transform-churn-experiment')\n",
    "\n",
    "cleanup_sme_sdk(experiment_to_cleanup)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
